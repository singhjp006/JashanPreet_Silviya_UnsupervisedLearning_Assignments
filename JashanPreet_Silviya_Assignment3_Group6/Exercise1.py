# -*- coding: utf-8 -*-
"""
Spyder Editor

@author: jashan
@author: silviyavelani
"""


# importing all the required modules.
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
import numpy as np
import pandas as pd
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics import silhouette_samples, silhouette_score
import matplotlib.cm as cm
from mpl_toolkits.mplot3d import Axes3D
from sklearn.decomposition import PCA
import scipy.cluster.hierarchy as sch
from sklearn.cluster import KMeans
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression


"""1. Retrieve and load the Olivetti faces dataset [5 points]"""
data, target = datasets.fetch_olivetti_faces(shuffle = True,
                                                     return_X_y = True)

"""2. Split the training set, a validation set, and a test set using stratified sampling 
to ensure that there are the same number of images per person in each set. """
X_train, X_test_validate, y_train, y_test_validate = train_test_split(data, target,
                                                    test_size = 0.40,
                                                    stratify = target);

X_test, X_validate, y_test, y_validate = train_test_split(X_test_validate, y_test_validate,
                                                    test_size = 0.50,
                                                    stratify = y_test_validate);

"""3. Using k-fold cross validation, train a classifier to predict which person is 
represented in each picture, and evaluate it on the validation set. [0 points]"""


LR_clf = LogisticRegression(multi_class="auto", solver="lbfgs", C = 1,
                            tol=1e-1, max_iter = 500)

scores = cross_val_score(LR_clf, X_validate, y_validate, cv=2, scoring="accuracy")
print("Scores on k-fold cross validation: ", scores)
print("Mean of Scores on k-fold cross validation: ", scores.mean())

LR_clf.fit(X_train, y_train)

print("Score of Logistic Regression classifier by using solver lbfgs: \n",
      LR_clf.score(X_validate, y_validate))

y_pred = LR_clf.predict(X_test)
print(accuracy_score(y_pred, y_test))


from sklearn.preprocessing import StandardScaler

#Scaled data using standardscaler and added it back in 'data' variable
scaler = StandardScaler()
scaler.fit(data)
X_scale = scaler.transform(data)
data = pd.DataFrame(X_scale)

from yellowbrick.cluster import KElbowVisualizer
model = AgglomerativeClustering()

visualizer = KElbowVisualizer(model, k=(2,170),metric='silhouette', timings= True)
visualizer.fit(data)        # Fit the data to the visualizer
visualizer.show() 




X = data.iloc[:].values

dendrogram = sch.dendrogram(sch.linkage(X, method='ward'))
plt.title("Actual Data Dendrogram")
plt.xlabel("X")
plt.ylabel("Eclidean Distances")
plt.show()


"""4. reduce the dimensionality of the set"""
#----Reduced dimensionality using PCA---#

pca = PCA() # create a PCA object
pca.fit(X_train) # do the math
train_pca = pca.transform(X_train) # get PCA coordinates for scaled_data

X_train_pca = train_pca[:,0:2]


dnd = sch.dendrogram(sch.linkage(X_train_pca, method='ward'))

#plt.scatter(X_train_pca[:,0], X_train_pca[:,1])

#Reducing deimensionality of validation data
val_pca = pca.transform(X_validate)

X_val_pca = val_pca[:,0:2]


"""4. Using Agglomerative Hierarchical Clustering (AHC) by using the following similarity measures:
a) Euclidean Distance [20 points]
b) Minkowski Distance [20 points]
c) Cosine Similarity [20 points]"""


"""Use the silhouette score approach to choose the number of clusters for 4(a)"""
print(f'\n\n\n----Euclidean Distance: ----')
for k in range(2,25):
    euclidean_clusterer = AgglomerativeClustering(n_clusters=k, affinity='euclidean', linkage='ward').fit(X_train_pca)
    print(f"KMeans Scaled Silhouette Score: for {k} --> {silhouette_score(X_train_pca, euclidean_clusterer.labels_, metric='euclidean')}")


euclidean_clusterer = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='ward')
y_predict_euclidean = euclidean_clusterer.fit_predict(X_train_pca)
euclidean_cluster_labels = euclidean_clusterer.labels_

#ploting 2 clusters generated by AHC with Euclidean distance
plt.scatter(X_train_pca[y_predict_euclidean==0,0], X_train_pca[y_predict_euclidean == 0,1], s=100, c= 'red')
plt.scatter(X_train_pca[y_predict_euclidean==1,0], X_train_pca[y_predict_euclidean == 1,1], s=100, c= 'blue')




"""Use the silhouette score approach to choose the number of clusters for 4(b)"""
print(f'\n\n\n----Minkowski Distance: ----')
for k in range(2,25):
    minkowski_clusterer = AgglomerativeClustering(n_clusters=k, affinity='minkowski', linkage='complete').fit(X_train_pca)
    print(f"KMeans Scaled Silhouette Score: for {k} --> {silhouette_score(X_train_pca, minkowski_clusterer.labels_, metric='minkowski')}")

minkowski_clusterer = AgglomerativeClustering(n_clusters=2, affinity='minkowski', linkage='complete')
y_predict_minkowski = minkowski_clusterer.fit_predict(X_train_pca)
minkowski_cluster_labels = minkowski_clusterer.labels_

#ploting 2 clusters generated by AHC with Euclidean distance
plt.scatter(X_train_pca[y_predict_minkowski==0,0], X_train_pca[y_predict_minkowski == 0,1], s=100, c= 'red')
plt.scatter(X_train_pca[y_predict_minkowski==1,0], X_train_pca[y_predict_minkowski == 1,1], s=100, c= 'blue')




"""Use the silhouette score approach to choose the number of clusters for 4(c)"""
print(f'\n\n\n----Cosine Distance: ----')
for k in range(2,50):
    cosine_clusterer = AgglomerativeClustering(n_clusters=k, affinity='cosine', linkage='complete').fit(X_train_pca)
    print(f"KMeans Scaled Silhouette Score: for {k} --> {silhouette_score(X_train_pca, cosine_clusterer.labels_, metric='cosine')}")

cosine_clusterer = AgglomerativeClustering(n_clusters=2, affinity='cosine', linkage='complete')
y_predict_cosine = cosine_clusterer.fit_predict(X_train_pca)
cosine_cluster_labels = cosine_clusterer.labels_

#ploting 2 clusters generated by AHC with Euclidean distance
plt.scatter(X_train_pca[y_predict_cosine==0,0], X_train_pca[y_predict_cosine == 0,1], s=100, c= 'red')
plt.scatter(X_train_pca[y_predict_cosine==1,0], X_train_pca[y_predict_cosine == 1,1], s=100, c= 'blue')




"""4. Using Agglomerative Hierarchical Clustering (AHC) and using the centroid-based clustering rule."""
kmeans_clustering = KMeans(n_clusters=2, n_init=100, max_iter=800, init='k-means++', random_state=42).fit(X_train_pca)

y_kmeans_clustering = kmeans_clustering.fit_predict(X_train_pca)

plt.scatter(X_train_pca[y_kmeans_clustering==0,0], X_train_pca[y_kmeans_clustering == 0,1], s=100, c= 'red')
plt.scatter(X_train_pca[y_kmeans_clustering==1,0], X_train_pca[y_kmeans_clustering == 1,1], s=100, c= 'blue')


"""Use the set from 4(a) to train a classifier as in (3) using k-fold cross validation. [30 points]"""
#Created object of logistic classifier 
lr_model = LogisticRegression(multi_class='multinomial',max_iter= 800)



#Cross validation
cv = KFold(n_splits=5, random_state=1, shuffle=True)
scores = cross_val_score(lr_model, X_val_pca, y_validate, scoring='accuracy', cv=cv, n_jobs=-1)


#fitted scaled trainning data with reduced dimensionality and labels that are converted 
#into 2 cluster using AHC with Euclidean

lr_model.fit(X_train_pca, y_predict_euclidean)


# Reduced dimensionality of test data 
test_pca = pca.transform(X_test)
X_test_pca = test_pca[:,0:2]

# Predicted values from logistic classifier on test data
y_pred = lr_model.predict(X_test_pca)

# To compare and find accuracy , converted original labels for test data into cluster
# Because our predicted value of test data are in clusters. Hence, we can compare both actual and predicted
y_test_ec = euclidean_clusterer.fit_predict(X_test_pca)

# Calculated accuracy score of prediction from logistic classifier
accuracy_score(y_pred, y_test_ec)
